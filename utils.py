import os 
import sys 
import time 
import math 
from collections import defaultdict, deque
import numpy as np 
import torch.nn as nn  
import torch
from functools import partial
from typing import Any, Callable, List, Tuple
### For Dataloader Inference 
from pathlib import Path
from torch.utils.data import Dataset, DataLoader
from torchvision import io, transforms
from sklearn.model_selection import train_test_split
from PIL import Image
#******************************************************
# Helper functions
#******************************************************

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

#******************************************************
# Loading Pre-trained Weights
#******************************************************

def load_pretrained_weights(model, pretrained_weights, checkpoint_key, model_name, patch_size): 
    
    '''
    model: ViT architecutre design with random itit Weight  
    pretrained_weights: The path of pretrained weights from your local machine 
    checkpoint_key:  If specific layer neeed loading check point ?? 
    model_name: provide name to loading checkpoint from MetaAI hub checkpoint if pretrained_weights is not provided
    patch_size: this argument provide the patch_size of pretrained model need to load

    '''
    
    if os.path.isfile(pretrained_weights): 
        state_dict= torch.load(pretrained_weights, map_location='cpu')
        if checkpoint_key is not None and checkpoint_key in state_dict: 
            print(f'take key {checkpoint_key} in provided checkpoint dict')
            state_dict= state_key[checkpoint_key]
        ## remove 'Module' prefix 
        state_dict= {k.replace("module.", ""): v for k,v in state_dict.items()}
        # remove 'backbone' prefix induced by multicrop wrapper 
        state_dict= {k.replace("backbone.", ""): v for k,v in state_dict.items()}
        msg= model.load_state_dict(state_dict, strict=False)
        print("Pretrained weight found at {}".format(pretrained_weights, msg))

    else: 
        print("Not using '--pretrained weights path', Loading Default Models")
        
        url= None 

        if model_name =="vit_small" and patch_size==16: 
            url="dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth"
        elif model_name == "vit_small" and patch_size == 8:
            url = "dino_deitsmall8_pretrain/dino_deitsmall8_pretrain.pth"
        elif model_name == "vit_base" and patch_size == 16:
            url = "dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth"
        elif model_name == "vit_base" and patch_size == 8:
            url = "dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth"
        elif model_name == "xcit_small_12_p16":
            url = "dino_xcit_small_12_p16_pretrain/dino_xcit_small_12_p16_pretrain.pth"
        elif model_name == "xcit_small_12_p8":
            url = "dino_xcit_small_12_p8_pretrain/dino_xcit_small_12_p8_pretrain.pth"
        elif model_name == "xcit_medium_24_p16":
            url = "dino_xcit_medium_24_p16_pretrain/dino_xcit_medium_24_p16_pretrain.pth"
        elif model_name == "xcit_medium_24_p8":
            url = "dino_xcit_medium_24_p8_pretrain/dino_xcit_medium_24_p8_pretrain.pth"
        elif model_name == "resnet50":
            url = "dino_resnet50_pretrain/dino_resnet50_pretrain.pth"
        if url is not None: 
            print("Since no pretrained weights have been provided, we load the reference pretrained DINO weights.")
            state_dict = torch.hub.load_state_dict_from_url(url="https://dl.fbaipublicfiles.com/dino/" + url)
            model.load_state_dict(state_dict, strict=True)

        else:
            print("There is no reference weights available for this model => We use random weights.")

def load_pretrained_linear_weights(linear_classifier, model_name, patch_size): 
    url= None 
    if model_name == "vit_small" and patch_size==16: 
         url = "dino_deitsmall16_pretrain/dino_deitsmall16_linearweights.pth"
    elif model_name == "vit_small" and patch_size == 8:
        url = "dino_deitsmall8_pretrain/dino_deitsmall8_linearweights.pth"
    elif model_name == "vit_base" and patch_size == 16:
        url = "dino_vitbase16_pretrain/dino_vitbase16_linearweights.pth"
    elif model_name == "vit_base" and patch_size == 8:
        url = "dino_vitbase8_pretrain/dino_vitbase8_linearweights.pth"
    elif model_name == "resnet50":
        url = "dino_resnet50_pretrain/dino_resnet50_linearweights.pth"
    if url is not None:
        print("We load the reference pretrained linear weights.")
        state_dict = torch.hub.load_state_dict_from_url(url="https://dl.fbaipublicfiles.com/dino/" + url)["state_dict"]
        linear_classifier.load_state_dict(state_dict, strict=True)
    else:
        print("We use random linear weights.")

class patch_head(nn.Module): 
    def __init__(self, in_dim, num_heads, k_num): 
        super().__init__()
        self.cls_token= nn.Parameter(torch.zeros(1, 1, in_dim))
        
        ## Adding the LayerScale Block CA
        #self.cls_blocks= nn.ModuleList([])
        # self.cls_blocks = nn.ModuleList([
        #     LayerScale_Block_CA(
        #         dim=in_dim, num_heads=num_heads, mlp_ratio=4.0, qkv_bias=True, qk_scale=None,
        #         drop=0.0, attn_drop=0.0, drop_path=0.0, norm_layer=partial(nn.LayerNorm, eps=1e-6),
        #         act_layer=nn.GELU, Attention_block=Class_Attention,
        #         Mlp_block=Mlp)
        #         for i in range(2)])

        trunc_normal_(self.cls_token, std=.02)
        self.norm= partial(nn.LayerNorm, eps=1e-6)(in_dim)
        self.apply(self._init_weights)
        self.k_num= k_num 
        self.k_size= 3
        self.loc224= self.get_local_index(196, self.k_size)
        self.loc96 = self.get_local_index(36, self.k_size)
        self.embed_dim=in_dim 

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
    def forward(self, x, loc=False): 
        cls_tokens= self.cls_token.expand(x.shape[0], -1. -1)

        if loc: 
            k_size= self.k_size 
            if x.shape[1]==196: 
                local_idx = self.loc224
            elif x.shape[1] == 36: 
                if self.k_size==14: 
                    k_size=6 
                local_idx = self.loc96 

            else: 
                print(x.shape)
                assert (False)

            ## X here will be Individual Patch (Batches 3, 16, 16)
            x_norm= nn.functional.normalize(x, dim=-1)
            ## Compute Cosine Similarity Matrix 
            sim_matrix= x_norm[:, local_idx] @ x_norm.unsqueeze(2).transpose(-2, -1)
            top_idx= sim_matrix.squeeze().topk(k= self.k_num, dim=-1)[1].view(-1, self.k_num, 1)

            x_loc= x[:, local_idx].view(-1, k_size**2-1, self.embed_dim)
            x_loc= torch.gather(x_loc, 1, top_idx.expand(-1, -1, self.embed_dim))
            for i, blk in enumerate(self.cls_blocks): 
                if i ==0: 
                    glo_tokens= blk(x, cls_tokens)
                    loc_tokens= blk(x_loc, cls_tokens.repeat(x.shape[1], 1, 1))
                else: 
                    glo_tokens= blk(x, glo_tokens)
                    loc_tokens= blk(x_loc, loc_tokens)
            loc_tokens= loc_tokens.view(x.shape)
            x= self.norm(torch.cat([glo_tokens, loc_tokens], dim=1))
        else: 
            for i, blk in enumerate(self.cls_blocks): 
                cls_tokens= blk(x, cls_tokens)
            x= self.norm(torch.cat([cls_tokens, x], dim=1))

        return x 


        @staticmethod 
        def get_local_index(N_patches, k_size): 
            loc_weight = [] 
            w = torch.LongTensor(list(range(int(math.sqrt(N_patches)))))
            ## Why we need to iterate through all patches
            for i in range(N_patches): 
                ix, iy = i //len(w), i%len(w) 
                wx= torch.zeros(int(math.sqrt(N_patches)))
                wy= torch.zeros(int(math.sqrt(N_patches)))
                wx[ix]=1 
                wy[iy]=1 
                ## Iteration through all N patches of Single Images?
                for j in range(1, int(k_size//2)+1 ): 
                    wx[(ix+j)%len(wx)]= 1 
                    wx[(ix-j)%len(wx)]= 1
                    wy[(iy+j)%len(wy)]= 1
                    wy[(iy-j)%len(wy)]= 1

                weight = (wy.unsqueeze(0)* wx.unsqueeze(1)).view(-1)
                weight[i] = 0 
                loc_weight.append(weight.nonzero().squeeze())
            
            return torch.stack(loc_weight)

#******************************************************
# Inference DataLoader
#******************************************************
class collatesingle_img: 

    def __call__(self, batch: List[torch.Tensor]) -> torch.FloatTensor:  
        return batch


class ImageOriginalData(Dataset):
    def __init__(self, files: List[str], img_size: int,transform_ImageNet=False): 
        self.files= files
        self.resize= transforms.Resize((img_size, img_size))
        self.transform_ImageNet= transform_ImageNet
        if self.transform_ImageNet: 
            print("Using imageNet normalization")
        self.transform_normal = transforms.Compose([
            transforms.Resize((img_size, img_size)),
            transforms.ToTensor(),
            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
        ])
    # Iterative through all images in dataste
    def __len__(self): 
        return len(self.files)

    def __getitem__(self, i): 
        with open(self.files[i], 'rb') as f:
            img = Image.open(f)
            img = img.convert('RGB')
        #img = io.read_image(self.files[i])
        # Checking the Image Channel
        # if img.shape[0] == 1: 
        #     img= torch.cat([img]*3)
        if self.transform_ImageNet:
            return self.transform_normal(img) 
        else: 
            return self.resize(img)


class normal_dataloader: 
    '''
    This normal dataloader loading dataset with *ONLY One Folder* 

    '''
    
    def __init__ (self, image_path, image_format="*.jpg", img_size= 224, batch_size=4, subset_data=0.2, transform_ImageNet=False ): 
        image_files_= [str(file) for file in Path(image_path).glob("*.jpg")]
        _,  self.image_files = train_test_split(image_files_, test_size=subset_data, random_state=42)
        
        self.img_size= img_size
        self.batch_size= batch_size
        self.transform_ImageNet=transform_ImageNet
    
    def val_dataloader(self): 
        val_data= ImageOriginalData(self.image_files, self.img_size,self.transform_ImageNet )
        print(f" total images in Demo Dataset: {len(val_data)}")
        val_dl= DataLoader(
            val_data, 
            self.batch_size*2, 
            shuffle=False, 
            drop_last= False, 
            num_workers=4, 
            pin_memory= True, 
            #collate_fn= collatesingle_img()
        )
        return val_dl 

