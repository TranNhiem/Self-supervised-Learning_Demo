
# Copyright 2022 TranNhiem.

# Code base Inherence from https://github.com/facebookresearch/dino/

# Permission is hereby granted, free of charge, to any person obtaining a copy of
# this software and associated documentation files (the "Software"), to deal in
# the Software without restriction, including without limitation the rights to use,
# copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the
# Software, and to permit persons to whom the Software is furnished to do so,
# subject to the following conditions:

# The above copyright notice and this permission notice shall be included in all copies
# or substantial portions of the Software.

import os
import sys
import time
import math
from collections import defaultdict, deque
import numpy as np
import torch.nn as nn
import torch
import torchvision
from functools import partial
from typing import Any, Callable, List, Tuple

# ******************************************************
# Helper functions
# ******************************************************

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

# ******************************************************
# Loading Pre-trained Weights
# ******************************************************

def load_pretrained_weights(model, pretrained_weights, checkpoint_key, model_name, patch_size):
    '''
    model: ViT architecutre design with random itit Weight  
    pretrained_weights: The path of pretrained weights from your local machine 
    checkpoint_key:  If specific layer neeed loading check point ?? 
    model_name: provide name to loading checkpoint from MetaAI hub checkpoint if pretrained_weights is not provided
    patch_size: this argument provide the patch_size of pretrained model need to load

    '''

    if os.path.isfile(pretrained_weights):
        state_dict = torch.load(pretrained_weights, map_location='cpu')
        if checkpoint_key is not None and checkpoint_key in state_dict:
            print(f'take key {checkpoint_key} in provided checkpoint dict')
            state_dict = state_dict[checkpoint_key]
        # remove 'Module' prefix
        state_dict = {k.replace("module.", ""): v for k,
                      v in state_dict.items()}
        # remove 'backbone' prefix induced by multicrop wrapper
        state_dict = {k.replace("backbone.", ""): v for k,
                      v in state_dict.items()}
        msg = model.load_state_dict(state_dict, strict=False)
        print(f"Pretrained weight found at {pretrained_weights, msg}")

    else:
        print("Not using '--pretrained weights path', Loading Default Models")

        url = None

        if model_name == "vit_small" and patch_size == 16:
            url = "dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth"
        elif model_name == "vit_small" and patch_size == 8:
            url = "dino_deitsmall8_pretrain/dino_deitsmall8_pretrain.pth"
        elif model_name == "vit_base" and patch_size == 16:
            url = "dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth"
        elif model_name == "vit_base" and patch_size == 8:
            url = "dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth"
        elif model_name == "xcit_small_12_p16":
            url = "dino_xcit_small_12_p16_pretrain/dino_xcit_small_12_p16_pretrain.pth"
        elif model_name == "xcit_small_12_p8":
            url = "dino_xcit_small_12_p8_pretrain/dino_xcit_small_12_p8_pretrain.pth"
        elif model_name == "xcit_medium_24_p16":
            url = "dino_xcit_medium_24_p16_pretrain/dino_xcit_medium_24_p16_pretrain.pth"
        elif model_name == "xcit_medium_24_p8":
            url = "dino_xcit_medium_24_p8_pretrain/dino_xcit_medium_24_p8_pretrain.pth"
        elif model_name == "resnet50":
            url = "dino_resnet50_pretrain/dino_resnet50_pretrain.pth"
        
        elif model_name == 'vit_base_ibot_16' and patch_size == 16:
            state_dict = torch.load('/home/rick/pretrained_weight/DINO_Weight/ViT_B_16_ckpt/checkpoint.pth', map_location='cpu')
            # remove 'Module' prefix
            state_dict = {k.replace("module.", ""): v for k,
                        v in state_dict.items()}
            # remove 'backbone' prefix induced by multicrop wrapper
            state_dict = {k.replace("backbone.", ""): v for k,
                        v in state_dict.items()}
            model.load_state_dict(state_dict, strict=False)
        
        elif model_name == 'vit_L_16_ibot' and patch_size == 16: 
            state_dict = torch.load('/home/rick/pretrained_weight/DINO_Weight/ViT_L_16_ckpt/checkpoint.pth', map_location='cpu')
            # remove 'Module' prefix
            state_dict = {k.replace("module.", ""): v for k,
                        v in state_dict.items()}
            # remove 'backbone' prefix induced by multicrop wrapper
            state_dict = {k.replace("backbone.", ""): v for k,
                        v in state_dict.items()}
            model.load_state_dict(state_dict, strict=False)
        
        if url is not None:
            print(
                "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.")
            state_dict = torch.hub.load_state_dict_from_url(
                url="https://dl.fbaipublicfiles.com/dino/" + url)
            model.load_state_dict(state_dict, strict=True)
        else:
            print(
                "There is no reference weights available for this model => We use random weights.")
        
        return model

def load_pretrained_linear_weights(linear_classifier, model_name, patch_size):
    url = None
    if model_name == "vit_small" and patch_size == 16:
        url = "dino_deitsmall16_pretrain/dino_deitsmall16_linearweights.pth"
    elif model_name == "vit_small" and patch_size == 8:
        url = "dino_deitsmall8_pretrain/dino_deitsmall8_linearweights.pth"
    elif model_name == "vit_base" and patch_size == 16:
        url = "dino_vitbase16_pretrain/dino_vitbase16_linearweights.pth"
    elif model_name == "vit_base" and patch_size == 8:
        url = "dino_vitbase8_pretrain/dino_vitbase8_linearweights.pth"
    elif model_name == "resnet50":
        url = "dino_resnet50_pretrain/dino_resnet50_linearweights.pth"
    if url is not None:
        print("We load the reference pretrained linear weights.")
        state_dict = torch.hub.load_state_dict_from_url(
            url="https://dl.fbaipublicfiles.com/dino/" + url)["state_dict"]
        linear_classifier.load_state_dict(state_dict, strict=True)
    else:
        print("We use random linear weights.")

